{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a machine identify a bee as a honey bee or a bumble bee? These bees have different behaviors and appearances, but given the variety of backgrounds, positions, and image resolutions, it can be a challenge for machines to tell them apart.\n",
    "\n",
    "Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data. Pollinating bees have critical roles in both ecology and agriculture, and diseases like colony collapse disorder threaten these species. Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import keras library\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# import Sequential from the keras models module\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# import Dense, Dropout, Flatten, Conv2D, MaxPooling2D from the keras layers module\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels.csv from datasets folder using pandas\n",
    "\n",
    "labels = pd.read_csv('\\labels.csv', index_col=None)\n",
    "labels = labels[labels['photo_quality'] == 1]\n",
    "\n",
    "# to keep only columns 'path', 'is_bee', 'label'\n",
    "labels = labels[['path', 'is_bee', 'label']]\n",
    "\n",
    "labels = labels[(labels['label'] == 'bee') | (labels['label'] == 'wasp')]\n",
    "labels.is_bee.value_counts() #0 - wasp, 1 - bee\n",
    "y = labels.is_bee\n",
    "\n",
    "# concate bees and wasps in 1 dataframe, include 1000 image of each genus\n",
    "def concat(labels):\n",
    "    labels_bee = labels[labels['label'] == 'bee'][:1000]\n",
    "    labels_wasp = labels[labels['label'] == 'wasp'][:1000]\n",
    "    labels_concat = pd.concat([labels_bee, labels_wasp], axis=0)\n",
    "    return labels_concat\n",
    "\n",
    "labels_concat = concat(labels)\n",
    "\n",
    "#Now we will import all images. Once imported, we will stack the resulting arrays into a single matrix and assign it to X.\n",
    "image_paths = list(labels_concat.path)\n",
    "image_new_paths = []\n",
    "\n",
    "#pipeline for resizing images\n",
    "def process_image(path):\n",
    "    #Load the image with Image.open and create paths to save our images to\n",
    "    img = Image.open('D:\\\\Python\\\\bee_or_wasp\\\\kaggle_bee_vs_wasp\\\\{}'.format(path))\n",
    "    \n",
    "     # create paths to save files to\n",
    "    rcz_path = \"D:\\\\Python\\\\bee_or_wasp\\\\kaggle_bee_vs_wasp\\\\saved_images\\\\rsz\\\\rsz_{}.jpg\".format(path.stem)\n",
    "    \n",
    "    #resize images and safe them to created path\n",
    "    rcz = img.resize((50, 50))\n",
    "    rcz.save(rcz_path)\n",
    "    \n",
    "    image_new_paths.append(rcz_path)\n",
    "    return image_new_paths\n",
    "    \n",
    "# for loop over image paths\n",
    "for img_path in image_paths:\n",
    "    process_image(Path(img_path))\n",
    "    \n",
    "labels_concat['rsz_path'] = image_new_paths\n",
    "\n",
    "#Â create empty list\n",
    "image_list = []\n",
    "\n",
    "for i in labels_concat.rsz_path:\n",
    "    # load image\n",
    "    img = io.imread(i).astype(np.float64)  \n",
    "    # append to list of all images\n",
    "    image_list.append(img)\n",
    "    \n",
    "# convert image list to single array\n",
    "X = np.array(image_list)\n",
    "y = labels_concat.is_bee\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out evaluation sets (x_eval and y_eval)\n",
    "x_interim, x_eval, y_interim, y_eval = train_test_split(X,\n",
    "                                           y,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=52)\n",
    "\n",
    "# split remaining data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_interim,\n",
    "                                                   y_interim, \n",
    "                                                   test_size=0.4,\n",
    "                                                   random_state=52)\n",
    "\n",
    "# initialize standard scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "def scale_features(train_features, test_features):\n",
    "    for image in train_features:\n",
    "        # for each channel, apply standard scaler's fit_transform method\n",
    "        for channel in range(image.shape[2]):\n",
    "            image[:, :, channel] = ss.fit_transform(image[:, :, channel])\n",
    "    for image in test_features:\n",
    "        # for each channel, apply standard scaler's transform method\n",
    "        for channel in range(image.shape[2]):\n",
    "            image[:, :, channel] = ss.fit_transform(image[:, :, channel])\n",
    "\n",
    "# apply scale_features to four sets of features\n",
    "scale_features(x_interim, x_eval)\n",
    "scale_features(x_train, x_test)\n",
    "\n",
    "# set model constants\n",
    "num_classes = 1\n",
    "\n",
    "# define model as Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer with 32 filters\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(50, 50, 3)))\n",
    "model.add(BatchNormalization())\n",
    "# add a second 2D convolutional layer with 64 filters\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# reduce dimensionality through max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# third convolutional layer with 64 filters\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# add dropout to prevent over fitting\n",
    "model.add(Dropout(0.25))\n",
    "# necessary flatten step preceeding dense layer\n",
    "model.add(Flatten())\n",
    "# fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# add additional dropout to prevent overfitting\n",
    "model.add(Dropout(0.50))\n",
    "\n",
    "# prediction layers\n",
    "model.add(Dense(num_classes, activation='sigmoid', name='preds'))\n",
    "\n",
    "# show model summary\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    # set the loss as binary_crossentropy\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    # set the optimizer as stochastic gradient descent\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "    # set the metric as accuracy\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# mock-train the model using the first ten observations of the train and test sets\n",
    "my_model = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the history from the training object\n",
    "history = my_model.history\n",
    "\n",
    "# Plot the training loss \n",
    "plt.plot(history['loss'], label='train_loss', color = 'blue')\n",
    "# Plot the validation loss\n",
    "plt.plot(history['val_loss'], label='validation_loss', color = 'green')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy\n",
    "plt.plot(history['accuracy'], label='accuracy', color = 'blue')\n",
    "# Plot the validation accyracy\n",
    "plt.plot(history['val_accuracy'], label='validation_accuracy', color = 'green')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "pretrained_cnn = model\n",
    "\n",
    "# evaluate model on test set\n",
    "score = pretrained_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# evaluate model on holdout set\n",
    "eval_score = pretrained_cnn.evaluate(x_eval, y_eval, verbose=0)\n",
    "# print loss score\n",
    "print('Eval loss:', eval_score[0])\n",
    "# print accuracy score\n",
    "print('Eval accuracy:', eval_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for x_eval\n",
    "y_proba = pretrained_cnn.predict(x_eval)\n",
    "\n",
    "print(\"First five probabilities:\")\n",
    "print(y_proba[:5])\n",
    "print(\"\")\n",
    "\n",
    "# predicted classes for x_eval\n",
    "y_pred = np.round(y_proba).astype('int')\n",
    "\n",
    "print(\"First five class predictions:\")\n",
    "print(y_pred[:5])\n",
    "print(\"\")\n",
    "\n",
    "#[f(x) if condition else g(x) for x in sequence]\n",
    "\n",
    "[print('bumble bee') if i == 1 else print('honey bee') for i in y_pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
